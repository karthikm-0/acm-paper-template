\begin{abstract}
    Robots are becoming increasingly capable of responding to humans' natural language instructions to perform everyday manipulation tasks such as wiping a table or meal preparation. However, natural language presents challenges, such as ambiguity in spatial instructions (e.g., choosing a specific apple from a basket) and requiring users to mentally track how the robot’s state evolves during long-horizon tasks. In this work, we propose using images as an alternative paradigm to instruct robots. We introduce \projname, a specific instantiation of this paradigm, which allows users to perform direct manipulation on images in a timeline-style interface to generate step-by-step robot instructions. Through a user study with twelve participants we evaluated \projname for instructing robots on kitchen manipulation tasks, comparing it to a text-based timeline. The results show that \projname was faster, led to fewer errors, and was preferred by participants over the text-based interface. We also demonstrate that image-based instructions can be translated into robot executable policies, and discuss the potential of combining the strengths of language and images to create multimodal robot instructions.

    % Robots are becoming increasingly capable of performing everyday tasks through modern foundation models, yet end-users still need to provide input to surface preferences and constraints to enable successful completion of tasks in their environments. Natural language is often considered an effective means for this two-way communication. However, it can be challenging for robots to communicate their actions in a way that is quickly understood and contextualized within the environment. Conversely, users may struggle to instruct the robot on changes to its task plans due to language ambiguities and the complexities of managing language over long-duration tasks. We propose a novel system, PhotoManipulator, which uses generated images to describe the robot’s task plans to the end-user. Through direct manipulation and interaction with the images of the task plans, PhotoManipulator surfaces user preferences and constraints that can be reused in future task iterations. Through user studies with X participants, we demonstrate that programs were easier to create and edit using PhotoManipulator when compared to a language-only interface.
\end{abstract}