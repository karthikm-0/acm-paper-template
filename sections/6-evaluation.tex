\section{Evaluation}
We evaluated \projname through a controlled user study with twelve participants recruited using university and professional networks. Specifically, we compare an instance of \projname to a language-based method. The choice of a language-based method for comparison is based on its widespread deployment to instruct robots~\cite{team2024octo, zha2023distilling}.

\emph{Conditions.} To enable a fair comparison of each modality, we omitted all the language features of \projname, such as captioning and manipulating images using text (\mk: add feature names). Further, we also excluded all the intelligent features in \projname including autocomplete at the manipulation and step levels. For the text condition, we re-purposed \projname and replacing all image-based interactions with text. Instead of populating images in the timeline, the user populates the timeline with steps that consist of text. For the image-based method in \projname, we omit all language-based features (e.g., modifying images with captions) as well as autocomplete at the manipulation and step levels. 

\emph{Study design.} The study utilizes a within-subjects design with two conditions--image and text--that are counterbalanced to minimize ordering effects (see Figure X for a full diagram). Within each condition, participants complete four tasks where they instruct a robot to complete kitchen manipulation tasks. The tasks include \textit{Organizing Pantry}, \textit{Sorting Fruits}, \textit{Preparing Stirfry}, and \textit{Washing Dishes}. Within each condition, the four tasks were randomly assigned. After both condition blocks, participants complete a freeform task to experiment with the features that were excluded from \projname. Details of the individual tasks can be found in the appendices (\mk{link}) and the website.

% To complete these tasks, the participant is provided the initial robot state as an image and a desired goal image. 

%%%% STATS about them like age, gender, etc. 

\emph{Measures.} In the study, we collected data about participants' performance when using both methods. Quantitative measures include task completion time and number of errors, which were determined by comparison to an \textit{oracle} representation of the task established a priori by two researchers. We also measured subjective perceptions of the interfaces, including participants' confidence in correctly communicating their intent to the robot, workload (NASA TLX~\cite{hart2006nasa}), and usability (SUS~\cite{bangor2008empirical}).

\mk{Procedure.} Participants first provided consent and completed a pre-study questionnaire assessing their familiarity with robots and instructing them. After a tutorial introducing \projname and the text-based method and brief experimentation with both interfaces, participants began one of the two study condition blocks. Between tasks, participants rated their confidence in communicating their intent using a 7-point Likert scale. At the end of each block, two questionnaires (NASA TLX and SUS) were administered to assess workload and usability, respectively. At the end of the study, we probed participants about their subjective preferences through an interview.