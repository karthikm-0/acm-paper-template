\section{Introduction}
\bla{Another random thought, but I think we should pick an application domain like household tasks and make it clear we are not stepping outside of that.}

\bla{I think this paragraph justifies the need for user intervention versus a fully autonomous ``interaction'', but I think it should instead spend one sentence justifying that, and more sentences justifying why more methods for creating and interpreting instructions for robots are needed. I.e. issues with representing instructions with language, or end user programming. I also think the reasons for needing user intervention are multifaceted, and more than just generic models don't adapt to user preferences. I think there's other issues such as models not actually performing well enough, and I think there's likely a user desire for more control over the model https://dl.acm.org/doi/abs/10.1145/3290605.3300750

I'd also maybe love a simpler, more straighforward and attention grabbing hook here.

``Advances in foundation models are improving the capabilities of autonomous robots at the pace of leaps and bounds instead of baby steps; we find ourselves on the cusp of having robots in our homes, in a world where we no longer have to do the dishes. (maybe some examples from the literature). However, the need for human instruction of robots is still needed. Whether it is limitations of the models, strict and unknown human preferences, a desire from humans for controllability, or simply the entry point for a human to instruct a robot, even once foundation models are in our homes, we will still need to instruct them.''}


%\mk{Computers \dots}
%The advancement of large foundation models that process combination of language and vision has made robots more intelligent and capable of performing complex tasks.
Robots are becoming increasingly autonomous and capable of performing complex tasks through the advent of robot foundation models. Thus, we expect that robots will aid us in completing everyday tasks in the near-term. While these models are trained on internet-scale data, they do not understand the specific environment they will operate in. Thus, they will still need to attend to instructions from users~\cite{ajaykumar2021survey}. Imagine a user asks a robot to put away newly purchased groceries in the kitchen. The robot, following a generic organisation procedure, might randomly store canned goods and cereals together based on the space available inside. However, the user might organize their cabinet in a more nuanced way, such as by placing heavier items on lower shelves due to their cabinet's fragility and lighter items above for easier lifting. They might also want daily-use items such as cereal in the front, even if it blocks other items. As a result, the user would need to intervene and adjust the robotâ€™s approach to match their environment and preferences.

\bla{If our goal is to position images in the landscape of techniques for instructing robots, I think we should stray away from comparing to language so much and instead make a general claim that the underlying problem we are solving are *many* drawbacks across different methods of instructing. For instance, interpreting end user programs is very difficult, language is difficult because of ambiguity, teleop requires an immense amount of concentration and has primarily been explored for real-time tasks not future tasks. The time spent manipulating for teleop is 1-1, so why wouldn't you do household tasks yourself.}

As it is expected that most users of future robots will not be experts, natural language has been touted as a modality that people can use to instruct robots~\cite{tellex2020robots}. From a technical standpoint, state-of-the-art robot policies often utilize language-conditioning to generate robot actions. Despite the ubiquity of language and its easy-to-use interface, language is not always ideal. Natural language can be abstract and difficult to ground in the physical context. For instance, \textit{``Put the cereal in the cabinet'' }is unclear if there are many boxes of cereal and several cabinets to put them away inside of. Hence, the user needs to be more precise to achieve the desired result: \textit{``Put the blue cereal box into the top left corner of the middle cabinet next to the other cereal''}, or alternatively, provide corrections to a simpler instruction by uttering language~\cite{zha2023distilling}. Beyond the immediate problem of specifying instructions, the user faces the additional challenge of predicting how the robot's planned actions will affect the environment, particularly for long-horizon tasks.

\bla{Again, too much focus on language if we are positioning the paper in the overall landscape of robot instruction techniques. ``In this paper we propose the idea of directly manipulating images of the environment as a means of  instructing a robot. Compared to existing methods, images are easy to interpret, and because of their concrete grounding in reality do not suffer from issues with ambiguity.'' the last sentence needs work. The key thing is pulling language out of the topic sentence, and instead focusing on addressing drawbacks of other instruction methods in the literature.

Overall I like this paragraph a lot though. Teasing out some of these axes along which images differ from other techniques will be super important.}

Instead of solely using natural language as a means for instructing robots, we propose the use of images---both as a means for providing instructions to a robot, and for the robot to communicate what its plans are to the user. Images provide two main benefits over language. First, images are inherently easier to ground, as they visually represent the environment from which they originate. Hence, users should be able to contextualize the robot's current and future states through images. Second, by forming an appropriate representation of the environment as an image, we can enable direct manipulation~\cite{shneiderman1983direct}, which could be faster than forming precise language to resolve ambiguity and provide clear instructions. In addition, by supporting direct manipulation, the robot can develop a representation of the user's intent informed by their actions, and use this to intelligently propose future appropriate actions.

\bla{This is great I love this section. I'd love some more details on features of the system here though, we can make room for it in the intro by tightening the paragraphs above.}

We introduce a specific instantiation of this paradigm, \projname, where users can manipulate images of the environment to create instructions for a robot. In a user study with twelve participants, we compared \projname to a language-based method for several instruction following tasks in simulated kitchen environments. We found that participants were able to generate instructions in these tasks faster and with less errors when using \projname, and participants were more confident that their instructions could be understood by a robot when directly manipulating images that our prototype supports. We also demonstrate that instructions created using \projname can be used to perform robot manipulation tasks on a physical robot arm setup. Lastly, we discuss how the two modalities of language and image can be used in a complementary fashion to create \textit{multimodal instructions}.


% Recently, researchers have explored different techniques to allow end users to provide clarifications and corrections to robots that plan using large language models (a class of FMs). In these systems, step-by-step procedures are represented as natural language, and the end user can provide feedback on the robot's procedure or actions through language~\cite{zha2023distilling, mahadevan2024generative}. 
% % Challenge

% % Hook
% Something about LLMs/VLMs/robotics that we now have access to intelligent robotic models. Now we can do NLP-based programming of robots!

% % Challenge
% Language on its own is insufficient and can have issues. Plus models these days are vision-based (not sure if we need to include this)

% % Contribution
% We propose a novel interface that treats image as a first-class citizen. Image for input/output/errors

% % Evaluation
% We evaluated BLAH BLAH using BLAH BLAH.

