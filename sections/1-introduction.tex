\section{Introduction}
%\mk{Computers \dots}
%The advancement of large foundation models that process combination of language and vision has made robots more intelligent and capable of performing complex tasks.
Robots are becoming increasingly autonomous and capable of performing complex tasks through the advent of robot foundation models. Thus, we expect that robots will aid us in completing everyday tasks in the near-term. While these models are trained on internet-scale data, they do not understand the specific environment they will operate in. Thus, they will still need to attend to instructions from users~\cite{ajaykumar2021survey}. Imagine a user asks a robot to put away newly purchased groceries in the kitchen. The robot, following attempt a generic organizing procedure, might randomly store canned goods and cereals together based on the space available inside. However, the user might organize their cabinet in a more nuanced way, such as by placing heavier items on lower shelves due to their cabinet's fragility and lighter items above for easier lifting. They might also want daily-use items such as cereal in the front, even if it blocks other items. As a result, the user would need to intervene and adjust the robotâ€™s approach to match their environment and preferences.

As it is expected that most users of future robots will not be experts, natural language has been touted as a modality that people can naturally use to instruct robots~\cite{tellex2020robots}. From a technical standpoint, state-of-the-art robot policies often utilize language-conditioning to generate robot actions. Despite the ubiquity of language and its easy-to-use interface, language is not always ideal. Natural language can be abstract and difficult to ground in the physical context. For instance, \textit{``Put the cereal in the cabinet'' }is unclear if there are many boxes of cereal and several cabinets to put them away inside of. Hence, the user needs to be more precise to achieve the desired result: \textit{``Put the blue cereal box into the top left corner of the middle cabinet next to the other cereal''}, or alternatively, provide corrections to a simpler instruction by uttering language~\cite{zha2023distilling}. Beyond the immediate problem of specifying instructions, the user faces the additional challenge of predicting how the robot's planned actions will affect the environment, particularly for long-horizon tasks.

Instead of solely using natural language as a means for instructing robots, we propose the use of images---both as a means for providing instructions to a robot, and for the robot to communicate what its plans are to the user. Images provide two main benefits over language. First, images are inherently easier to ground, as they visually represent the environment from which they originate. Hence, users should be able to contextualize the robot's current and future states through images. Second, by forming an appropriate representation of the environment as an image, we can enable direct manipulation~\cite{shneiderman1983direct}, which could be faster than forming precise language to resolve ambiguity and provide clear instructions. Finally, by supporting direct manipulation, the robot can develop a representation of the user's intent informed by their actions, and use this to intelligently propose future appropriate actions.

We introduce a specific instantiation of this paradigm, \projname, where users can manipulate images of the environment to create instructions for a robot. In a user study with twelve participants, we compared \projname to a language-based method for several instruction following tasks in simulated kitchen environments. We found that participants were able to generate instructions in these tasks faster and with less errors when using \projname, and participants were more confident that their instructions could be understood by a robot when directly manipulating images that our prototype supports. We also demonstrate that instructions created using \projname can be used to perform robot manipulation tasks on a physical robot arm setup. Lastly, we discuss how the two modalities of language and image can be used in a complementary fashion to create \textit{multimodal instructions}.


% Recently, researchers have explored different techniques to allow end users to provide clarifications and corrections to robots that plan using large language models (a class of FMs). In these systems, step-by-step procedures are represented as natural language, and the end user can provide feedback on the robot's procedure or actions through language~\cite{zha2023distilling, mahadevan2024generative}. 
% % Challenge

% % Hook
% Something about LLMs/VLMs/robotics that we now have access to intelligent robotic models. Now we can do NLP-based programming of robots!

% % Challenge
% Language on its own is insufficient and can have issues. Plus models these days are vision-based (not sure if we need to include this)

% % Contribution
% We propose a novel interface that treats image as a first-class citizen. Image for input/output/errors

% % Evaluation
% We evaluated BLAH BLAH using BLAH BLAH.

