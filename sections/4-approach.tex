\section{\projname Interactions}
Our system assumes a set-up phase where the robot builds an internal representation of the user's environment. This representation consists of objects that can be stateless or fixtures that are stateful (i.e., take on states). For instance, some items, like bowls, are \textit{objects} because they do not have states (but can vary in location or contents). Other items, like furniture (e.g., drawers) or appliances (e.g., stovetops) are \textit{fixtures} because they can be in one or more states (e.g., open or closed in the case of drawers). In \projname, the user interact with objects and fixtures in different ways. Imagine that a user wants to wash dishes after a meal. We walk through the interactions that the user might have with \projname to instruct the robot to complete this task.

%In our specific implementation, stateless objects are called \emph{items}, such as food (e.g., fruits) and utensils (e.g., bowls). \mk{Figure}. In contrast, stateful objects are called \emph{fixtures} which includes furniture (e.g,, cabinets and drawers) as well as appliances (e.g., stovetops or sinks). For instance, a drawer may be open or closed, and a stovetop may have any of its burners on or off. Once this representation is created, \projname keeps track of the current state as well as possible future states.

% In our implementation, we assume access to the initial state of fixtures (but not objects) as well as knowledge about the states they can possible take (e.g,, that a drawer can be open or closed). 

% Define step here as it pertains to user (not robot)
\emph{Timeline.} The user interacts with \projname through a timeline that represents the current state of the environment as well as the desired changes. When the user starts \projname, a step representing the current world state populates the timeline. All future steps represent changes from this initial step which the robot treats as instructions. In the timeline, images for each step are displayed from left to right and visualized as smaller thumbnails. Each step includes buttons that allow the user to copy or delete the step. Copying a step creates a new step with the same image, which the user can then modify. Deleting a step removes it from the timeline.

\emph{Editor.} To support the creation of new instructions, the user can click any step in the timeline. This highlights the step in the timneline and populates it inside an image editor which appears above the timeline. The editor supports the manipulation of objects and fixtures in the environment.

% \emph{Defining steps.} In \projname, a \textit{step} represents a desired change requested by the user to update the environment's state. Suppose the user wants the robot to put a dirty dish on the counter into the sink for rinsing. Steps here are defined as changes from the user's perspective. Thus, a possible next step could be for the dirty dish to be on the counter. Note that for a robot, executing this single step requires several actions: the robot needs to first pick up the dirty dish, and then place it in the sink.

\emph{Interactable objects.} Once the editor appears, the user can perform simple drag and drop manipulations on recognized objects. When the user manipulates an object, it automatically populates a step in the timeline to the right of the selected step, and populates the editor with the new image. Continuously manipulating the same object does not create additional steps, which allows the user to refine the object's position without creating unnecessary steps.

\emph{Interactable fixtures.} In \projname, the user can manipulate the state of recognized fixtures by clicking on them. This triggers the creation of a new step where the image represents the environment with the fixture's state having updated.

\emph{Tracking changes between steps.} To help the user understand the changes between steps, \projname implements two visualization techniques. First, to quickly delineate changes between one step and the next, \projname highlights the differences between the two images. When the change involves an object moving, it is highlighted while other objects and the background containing fixtures are lower in opacity. Second, when the user hovers over a step in the timeline while having a step selected, a looping animation shows the changes between them. For object manipulations, the animation shows the object moving from one position to another. For fixture manipulations, the animation shows the fixture transitioning between states.

\emph{Captioning steps.} To provide additional context to the user, \projname automatically generates captions for each step in the timeline. These captions describe the changes between the current step and the previous step. For instance, if the user moves a small plate among other plates on the counter into the sink, the caption might read \textit{``Move the small plate from the left of the counter into the sink''}. In some cases, the user may not be satisfied with their manipulation and may choose to modify the caption. By default, this modifies the image in the step to reflect the new caption. In other cases, the user might be satisfied with the visual outcome of their manipulation but not the caption. In this case, the user can modify the caption without changing the image by unlinking the caption from the image.

\emph{Language-based editing.} In addition to direct manipulation, the user can also use natural language to request state changes. When a step is selected in the timeline and appears in the editor, the user can input an instruction using natural language. Upon submitting their input, \projname can generate one or many steps in the timeline that reflect the user's instruction. For instance, if the user types \textit{``Move the small plate from the left of the counter into the sink''}, \projname generates a step that shows the small plate moving from the counter to the sink. 

\emph{Predicting future steps.} As the user adds new steps to the timeline, \projname attempts to recognize the user's end goals (e.g., to clean up after meal preparation). Using this knowledge, it automatically proposes steps to the user in the timeline. Proposed steps are visualized as semi-transparent to the user and can be added to the timeline by clicking them or rejected by pressing the ``x'' button that appears on the top-left. 

%%%% Types of autocomplete
% Proposing next steps based on existing steps - useful
% Using object selection or current step to propose text (when using input bar) - less useful
% Taking an interaction done by the user (e.g., dragging plate to sink) and proposing an action (drag x to y) automatically (ripped from Damien)
% Proposing the step when an item is selected inside the current step (e.g., plate selected -> can go to cabinet, sink, counter??)


%% What interactions are possible; for now drag/drop + click to change states


% Reg drag and drop 

% Articulated objects



% Autocomplete features


%\subsection{GIFs}
